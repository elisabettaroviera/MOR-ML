{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJTl4XquQLgi"
   },
   "source": [
    "# **Convolutional Neural Networks**\n",
    "\n",
    "**Disclaimer**: large parts of the lab are taken from [here](https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/).\n",
    "\n",
    "**Definition**: convolutional neural networks (CNNs) are usually used for image recognition. A network is a convolutional network if it uses _convolution_ at least in one layer (the convolutional layer).\n",
    "\n",
    "**IDEA**: start from an immagise, and color channel goes the right color of the pixel that you are analyzing. You enlarge the space so that the NET is capable to understed which are the most important & usefull features. The NET has fullfill the space, so that he can analyze better and find the features. At the end, I have to have only a vector - containing classes.\n",
    "\n",
    "<img src=\"./Images/CNN_1.png\"\n",
    " style=\"float:center;\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgMHPb6bRl3Q"
   },
   "source": [
    "**The convolutional layer**: the **convolutional layer _extracts features_** (sharpen, blur...) from the pixels. It is a mathematical operation between the input image and the kernel (filter).  --> application of the kernel\n",
    "\n",
    "This is an example of **poling** filter that compress the information that I have. The idea is that I enlarge at the beginning, but after I have to reduce the dimension so that I can effort the computation cost. Here, i.e.,  we are applying moltiplication (as we can see in the photo).\n",
    "<img src=\"./Images/CNN_2.png\"\n",
    " style=\"float:center;\" align=\"center\">\n",
    "\n",
    " [Credits](https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bupBuk4gVOTY"
   },
   "source": [
    "**The pool layer**: pooling layers reduce the dimensionality of the previous layer, mantaining the most important features. Common choices are the max and the average value. --> apply only in the right example: you have to understand wich layer put in your net in relation to what you have to do, what is your purpose.\n",
    "\n",
    "<img src=\"./Images/CNN_3.png\"\n",
    " style=\"float:center;\" align=\"center\">\n",
    "\n",
    "\n",
    " [Credits](https://www.researchgate.net/publication/333593451_Application_of_Transfer_Learning_Using_Convolutional_Neural_Network_Method_for_Early_Detection_of_Terry%27s_Nail/figures?lo=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeaAAV_dnrlj"
   },
   "source": [
    "Let us try to build our first CNN!\n",
    "As usual, first thing first, we import the libraries and we ask for GPUs.\n",
    "\n",
    "**AIM** Works with colored images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WX_xp_pnonu9"
   },
   "outputs": [],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPjnYi3GovzO"
   },
   "source": [
    "Let us define some useful parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iPICtUHuFXrf"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8IZ4AJKppDD"
   },
   "source": [
    "Let us import the Dataset.\n",
    "The below code:\n",
    "* defines a transformation over the data (resizes, converts and normalizes),\n",
    "* downloads the data,\n",
    "* defines dataloaders which load the data in batches (the RAM does not suffer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6NEGVhd0pbh-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 170M/170M [00:17<00:00, 9.88MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Use transforms.compose method to reformat images for modeling,\n",
    "# and save to variable all_transforms for later use\n",
    "\n",
    "# We compose the transformation, we have 3 step in it: Resize, ToTensor and Normalize (3 because one for each color - channel)\n",
    "all_transforms = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                                          std=[0.2023, 0.1994, 0.2010])\n",
    "                                     ])\n",
    "# Create Training dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                             train = True,\n",
    "                                             transform = all_transforms,\n",
    "                                             download = True)\n",
    "\n",
    "# Create Testing dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                            train = False,\n",
    "                                            transform = all_transforms,\n",
    "                                            download=True)\n",
    "\n",
    "# Instantiate loader objects to facilitate processing\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zw-eqyJlraMM"
   },
   "source": [
    "We have to create the CNN and, as usual, we extend the ``nn.Module`` and define the layers in the constructor.\n",
    "\n",
    "Then, we have to define the ``forward`` method. The input channel is 3 (3 channels: red, green and blue). We are \"enlarging\" the third dimension to captures more features (opacity, sharpness...).\n",
    "\n",
    "Then we pool. You define how many pixel you want to consider (2x2), filter 2-by-2, and the _stride_, i.e. how do you move along the pixels.\n",
    "\n",
    "In the end, a fully connected layer is defined (nothing new).\n",
    "\n",
    "\n",
    "**IMP - Courious about how to match dimensions of the CNN?** [Give a look](https://towardsdatascience.com/a-comprehensible-explanation-of-the-dimensions-in-cnns-841dba49df5e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pOmnDGTptA8"
   },
   "outputs": [],
   "source": [
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "\t#  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "\n",
    "        # Different dimension that are power of 2, depends on the image and channel that you want\n",
    "        # NB You have to match the dimension of the matrix moltiplication in each step of the convolutional layer\n",
    "\n",
    "        # Build the layer\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3) # Convolutional layer, stride omitted == takes all the stuff\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3) # Convolutional layer\n",
    "        \n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2) # Max poll layer: kernel_size is the dimension of the filter, stride represent how many step I'm doing through the kernel\n",
    "        \n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3) # Convolutional layer \"out_channels\" here we enlarge the dimension\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3) # Convolutional layer\n",
    "        \n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2) # Max pool \n",
    "        \n",
    "        self.fc1 = nn.Linear(1600, 128) # Linear layer - tricky part to match the dimension\n",
    "        \n",
    "        self.relu1 = nn.ReLU() # Activation function - you have to call it to activate the layer\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, num_classes) # Linear layer\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        # Structur of the net - see the different layer\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "        \n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        \n",
    "        out = self.max_pool2(out)\n",
    "                \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iedRghgMJmNv"
   },
   "source": [
    "Let us define the loss and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy2t_-4Txkq1"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = ConvNeuralNet(num_classes)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss() # We select Entropy because we're working with classification problem\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9) # Stocastic gradient descent\n",
    "# weight_decay := the parameter of the penalication of the weight (you do NOT want the weight increase too mucn) --> otherwise, when you apply the loss\n",
    "# maybe you are just focusing on the value of the weigths and not on the athore part of the minimization function\n",
    "# momentum := related to the gradient computed in the prevoius step\n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF-mc6SDJ5hJ"
   },
   "source": [
    "Let us train the model and test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUDrmtOqx_L1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.6079\n",
      "Epoch [2/20], Loss: 1.6913\n",
      "Epoch [3/20], Loss: 1.7127\n",
      "Epoch [4/20], Loss: 1.6160\n",
      "Epoch [5/20], Loss: 1.1036\n",
      "Epoch [6/20], Loss: 1.1968\n",
      "Epoch [7/20], Loss: 1.3355\n",
      "Epoch [8/20], Loss: 0.9578\n",
      "Epoch [9/20], Loss: 1.4896\n",
      "Epoch [10/20], Loss: 0.7365\n",
      "Epoch [11/20], Loss: 0.7428\n",
      "Epoch [12/20], Loss: 0.8093\n",
      "Epoch [13/20], Loss: 0.5475\n",
      "Epoch [14/20], Loss: 0.7854\n",
      "Epoch [15/20], Loss: 0.6036\n",
      "Epoch [16/20], Loss: 0.7846\n",
      "Epoch [17/20], Loss: 0.8605\n",
      "Epoch [18/20], Loss: 0.7052\n",
      "Epoch [19/20], Loss: 0.4417\n",
      "Epoch [20/20], Loss: 0.6482\n"
     ]
    }
   ],
   "source": [
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "\t#Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device) # Using cpu or gpu in this data\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # Put zero the value at the beginning\n",
    "        loss.backward()\n",
    "        optimizer.step() # Optimize \n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XsHnJW6JYMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50000 train images: 82.378 %\n"
     ]
    }
   ],
   "source": [
    "# How we compute the accurcy of the network\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the network on the {} train images: {} %'.format(50000, 100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
